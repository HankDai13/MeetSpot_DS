#!/usr/bin/env python3
"""
Campus Data Generator with Real Amap API Data

Fetches real POI data from Amap API for Xiamen University campuses:
- æ€æ˜æ ¡åŒº (Siming Campus)
- ç¿”å®‰æ ¡åŒº (Xiang'an Campus)

Uses LLM to classify POIs into categories (CafÃ©, Library, Canteen).

Usage:
    python tools/generate_campus_data_real.py

Requires:
    - AMAP_API_KEY in config/config.toml or environment variable
"""

import asyncio
import json
import math
import os
import sys
from collections import deque
from typing import Dict, List, Optional, Tuple

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

import aiohttp

# Web service API key for server-side requests
# The config file api_key is web-only, so we use the web service key directly
AMAP_API_KEY = os.getenv("AMAP_WEB_SERVICE_KEY", "c652c6974305500cae8c408d1cfcc161")


async def geocode_location(session: aiohttp.ClientSession, address: str) -> Optional[Tuple[float, float]]:
    """Get coordinates for an address using Amap geocoding API."""
    url = "https://restapi.amap.com/v3/geocode/geo"
    params = {
        "key": AMAP_API_KEY,
        "address": address,
        "output": "json"
    }
    
    try:
        async with session.get(url, params=params) as resp:
            data = await resp.json()
            if data.get("status") == "1" and data.get("geocodes"):
                location = data["geocodes"][0]["location"]
                lng, lat = location.split(",")
                return (float(lat), float(lng))
    except Exception as e:
        print(f"Geocoding error for '{address}': {e}")
    
    return None


async def search_pois(
    session: aiohttp.ClientSession,
    center: Tuple[float, float],
    radius: int = 2000,
    types: str = "",
    keywords: str = ""
) -> List[Dict]:
    """Search POIs around a location using Amap API."""
    url = "https://restapi.amap.com/v3/place/around"
    all_pois = []
    
    # Paginate through results
    for page in range(1, 4):  # Max 3 pages (60 POIs)
        params = {
            "key": AMAP_API_KEY,
            "location": f"{center[1]},{center[0]}",  # lng,lat format
            "radius": radius,
            "offset": 25,
            "page": page,
            "output": "json",
            "extensions": "all"
        }
        
        if types:
            params["types"] = types
        if keywords:
            params["keywords"] = keywords
        
        # Rate limiting: 3 QPS limit -> sleep 0.4s
        await asyncio.sleep(0.4)
        
        try:
            async with session.get(url, params=params) as resp:
                data = await resp.json()
                status = data.get("status")
                info = data.get("info", "")
                
                if status != "1":
                    print(f"    API Error: status={status}, info={info}")
                    # If rate limited, wait longer and retry once
                    if "LIMIT" in str(info):
                        print("    âš ï¸ Rate limit hit, waiting 2s...")
                        await asyncio.sleep(2.0)
                        continue
                    break
                
                pois = data.get("pois", [])
                if pois:
                    all_pois.extend(pois)
                    count = int(data.get("count", 0))
                    if len(all_pois) >= count:
                        break
                else:
                    break
        except Exception as e:
            print(f"    POI search error: {e}")
            break
    
    return all_pois


def classify_poi(poi: Dict) -> str:
    """Classify a POI into our categories based on type and name."""
    type_code = poi.get("typecode", "")
    name = poi.get("name", "").lower()
    poi_type = poi.get("type", "").lower()
    
    # CafÃ© detection
    cafe_keywords = ["å’–å•¡", "coffee", "cafÃ©", "æ˜Ÿå·´å…‹", "ç‘å¹¸", "costa", "èŒ¶", "å¥¶èŒ¶", "ä¹¦å’–"]
    if any(kw in name or kw in poi_type for kw in cafe_keywords):
        return "CafÃ©"
    
    # Library detection
    library_keywords = ["å›¾ä¹¦é¦†", "å›¾ä¹¦å®¤", "é˜…è§ˆå®¤", "è‡ªä¹ ", "library", "èµ„æ–™å®¤", "ä¹¦åº—"]
    if any(kw in name or kw in poi_type for kw in library_keywords):
        return "Library"
    
    # Canteen detection
    canteen_keywords = ["é£Ÿå ‚", "é¤å…", "é¤é¥®", "é¥­å ‚", "ç¾é£Ÿ", "å°åƒ", "å¿«é¤", "canteen"]
    if any(kw in name or kw in poi_type for kw in canteen_keywords):
        return "Canteen"
    
    # Type code based classification
    if type_code.startswith("050"):  # é¤é¥®æœåŠ¡
        return "Canteen"
    if type_code.startswith("14"):  # ç§‘æ•™æ–‡åŒ–
        return "Library"
    
    return "Other"


def haversine_distance(lat1: float, lng1: float, lat2: float, lng2: float) -> float:
    """Calculate distance between two points in meters."""
    R = 6371000
    phi1, phi2 = math.radians(lat1), math.radians(lat2)
    delta_phi = math.radians(lat2 - lat1)
    delta_lambda = math.radians(lng2 - lng1)
    
    a = math.sin(delta_phi / 2) ** 2 + \
        math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda / 2) ** 2
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    
    return R * c


def generate_nodes_from_pois(pois: List[Dict], campus: str) -> List[Dict]:
    """Generate road nodes from POI locations."""
    nodes = []
    seen_locations = set()
    node_id = 1 if campus == "æ€æ˜" else 100
    
    for poi in pois:
        location = poi.get("location", "")
        if not location or location in seen_locations:
            continue
        
        try:
            lng, lat = map(float, location.split(","))
            # Round to create a grid-like structure
            grid_lat = round(lat, 4)
            grid_lng = round(lng, 4)
            grid_key = f"{grid_lat},{grid_lng}"
            
            if grid_key in seen_locations:
                continue
            seen_locations.add(grid_key)
            
            nodes.append({
                "id": f"N{node_id:03d}",
                "lat": grid_lat,
                "lng": grid_lng,
                "name": poi.get("name", f"{campus}è·¯å£{node_id}"),
                "campus": campus
            })
            node_id += 1
            
        except (ValueError, AttributeError):
            continue
    
    return nodes


def generate_edges(nodes: List[Dict], max_distance: float = 500) -> List[Dict]:
    """Generate edges between nearby nodes."""
    edges = []
    
    for i, node1 in enumerate(nodes):
        for node2 in nodes[i+1:]:
            # Only connect nodes in the same campus
            if node1.get("campus") != node2.get("campus"):
                continue
            
            dist = haversine_distance(
                node1["lat"], node1["lng"],
                node2["lat"], node2["lng"]
            )
            
            if dist <= max_distance:
                edges.append({
                    "from": node1["id"],
                    "to": node2["id"],
                    "weight": round(dist, 1)
                })
    
    return edges


def ensure_connectivity(nodes: List[Dict], edges: List[Dict]) -> List[Dict]:
    """Ensure the graph is connected within each campus."""
    for campus in ["æ€æ˜", "ç¿”å®‰"]:
        campus_nodes = [n for n in nodes if n.get("campus") == campus]
        if len(campus_nodes) < 2:
            continue
        
        node_ids = {n["id"] for n in campus_nodes}
        node_map = {n["id"]: n for n in campus_nodes}
        
        # Build adjacency
        adj = {nid: set() for nid in node_ids}
        for edge in edges:
            if edge["from"] in node_ids and edge["to"] in node_ids:
                adj[edge["from"]].add(edge["to"])
                adj[edge["to"]].add(edge["from"])
        
        # Find components
        visited = set()
        components = []
        
        for start in node_ids:
            if start in visited:
                continue
            component = set()
            queue = deque([start])
            while queue:
                node = queue.popleft()
                if node in visited:
                    continue
                visited.add(node)
                component.add(node)
                for neighbor in adj[node]:
                    if neighbor not in visited:
                        queue.append(neighbor)
            components.append(component)
        
        # Connect components
        for i in range(len(components) - 1):
            min_dist = float('inf')
            best_pair = None
            
            for n1 in components[i]:
                for n2 in components[i + 1]:
                    dist = haversine_distance(
                        node_map[n1]["lat"], node_map[n1]["lng"],
                        node_map[n2]["lat"], node_map[n2]["lng"]
                    )
                    if dist < min_dist:
                        min_dist = dist
                        best_pair = (n1, n2)
            
            if best_pair:
                edges.append({
                    "from": best_pair[0],
                    "to": best_pair[1],
                    "weight": round(min_dist, 1)
                })
    
    return edges


async def main():
    """Main entry point."""
    print("=" * 60)
    print("å¦é—¨å¤§å­¦æ ¡å›­æ•°æ®ç”Ÿæˆå™¨ (Amap API)")
    print("=" * 60)
    print(f"API Key: {AMAP_API_KEY[:8]}...{AMAP_API_KEY[-4:]}")
    print()
    
    async with aiohttp.ClientSession() as session:
        # Step 1: Geocode campus locations
        print("ğŸ“ è·å–æ ¡åŒºåæ ‡...")
        
        siming_coords = await geocode_location(session, "å¦é—¨å¤§å­¦æ€æ˜æ ¡åŒº")
        xiangan_coords = await geocode_location(session, "å¦é—¨å¤§å­¦ç¿”å®‰æ ¡åŒº")
        
        if not siming_coords:
            siming_coords = (24.436084, 118.101683)
        print(f"  âœ… æ€æ˜æ ¡åŒº: {siming_coords}")
        
        if not xiangan_coords:
            xiangan_coords = (24.608429, 118.309669)
        print(f"  âœ… ç¿”å®‰æ ¡åŒº: {xiangan_coords}")
        
        # Step 2: Search POIs
        all_pois = []
        
        # --- ç¿”å®‰æ ¡åŒºå…¨é¢æœç´¢ (Grid Search) ---
        print(f"\nğŸ” æœç´¢ ç¿”å®‰æ ¡åŒº POI (å…¨é¢æ¨¡å¼)...")
        xiangan_pois = []
        seen_ids = set()
        
        # Grid definition for Xiangan (roughly 2km x 2km box around center)
        # Center: 24.608429, 118.309669
        # Delta ~0.01 degrees is roughly 1km
        grid_points = []
        base_lat, base_lng = xiangan_coords
        for lat_offset in [-0.008, 0, 0.008]:
            for lng_offset in [-0.008, 0, 0.008]:
                grid_points.append((base_lat + lat_offset, base_lng + lng_offset))
        
        # Keywords for comprehensive building search
        xiangan_keywords = [
            # Functional
            "é¤å…", "é£Ÿå ‚", "å’–å•¡", "å›¾ä¹¦é¦†", "è¶…å¸‚", "ä¾¿åˆ©åº—", "æ‰“å°",
            # Academic & Buildings
            "æ¥¼", "å­¦é™¢", "ä¸­å¿ƒ", "å®éªŒå®¤", "ç ”ç©¶é™¢", "å­¦ç”Ÿæ´»åŠ¨ä¸­å¿ƒ",
            # Living
            "å®¿èˆ", "å…¬å¯“", "å›­åŒº", "ä½“è‚²é¦†", "è¿åŠ¨åœº", "æ¸¸æ³³é¦†"
        ]
        
        total_grids = len(grid_points)
        for i, grid_center in enumerate(grid_points):
            print(f"  Grid {i+1}/{total_grids}: {grid_center}")
            # Search broadly in each grid
            for kw in xiangan_keywords:
                pois = await search_pois(session, grid_center, radius=1000, keywords=kw)
                new_count = 0
                for poi in pois:
                    poi_id = poi.get("id")
                    if poi_id and poi_id not in seen_ids:
                        # Filter to ensure it's actually XMU related (optional, but good for noise reduction)
                        name = poi.get("name", "")
                        # Simple spatial filtering check (is it roughly near campus?)
                        # But for now rely on Amap's proximity
                        seen_ids.add(poi_id)
                        poi["_campus"] = "ç¿”å®‰"
                        xiangan_pois.append(poi)
                        new_count += 1
                # print(f"    + {kw}: {new_count}")

        print(f"  ç¿”å®‰æ ¡åŒºæ€»è®¡æ‰¾åˆ° {len(xiangan_pois)} ä¸ª POI")
        all_pois.extend(xiangan_pois)


        # --- æ€æ˜æ ¡åŒºæ ‡å‡†æœç´¢ ---
        print(f"\nğŸ” æœç´¢ æ€æ˜æ ¡åŒº POI...")
        siming_pois = []
        search_configs = [
            ("", ""),         # All POIs
            ("", "å’–å•¡"),
            ("", "é¤å…"),
            ("", "é£Ÿå ‚"),
            ("", "å›¾ä¹¦é¦†"),
        ]
        
        for types, keywords in search_configs:
            pois = await search_pois(session, siming_coords, radius=1500, types=types, keywords=keywords)
            for poi in pois:
                poi_id = poi.get("id")
                if poi_id and poi_id not in seen_ids:
                    seen_ids.add(poi_id)
                    poi["_campus"] = "æ€æ˜"
                    siming_pois.append(poi)
                    all_pois.append(poi)
        
        print(f"  æ€æ˜æ ¡åŒºæ‰¾åˆ° {len(siming_pois)} ä¸ª POI")
        
        # Step 3: Classify POIs
        print(f"\nğŸ·ï¸ åˆ†ç±» POI...")
        classified_pois = []
        category_counts = {"CafÃ©": 0, "Library": 0, "Canteen": 0, "Building": 0, "Other": 0}
        
        for poi in all_pois:
            # Enhanced classification
            name = poi.get("name", "").lower()
            poi_type = poi.get("type", "").lower()
            category = "Other"
            
            # 1. CafÃ©
            if any(kw in name or kw in poi_type for kw in ["å’–å•¡", "coffee", "cafÃ©", "æ˜Ÿå·´å…‹", "ç‘å¹¸", "èŒ¶", "é¥®å“"]):
                category = "CafÃ©"
            # 2. Library/Study
            elif any(kw in name or kw in poi_type for kw in ["å›¾ä¹¦é¦†", "é˜…è§ˆå®¤", "è‡ªä¹ ", "ä¹¦åº—"]):
                category = "Library"
            # 3. Canteen/Food
            elif any(kw in name or kw in poi_type for kw in ["é£Ÿå ‚", "é¤å…", "é¤é¥®", "ç¾é£Ÿ", "å°åƒ"]):
                category = "Canteen"
            # 4. Academic/Dorm Buildings (New)
            elif any(kw in name for kw in ["æ¥¼", "å­¦é™¢", "ä¸­å¿ƒ", "å®éªŒå®¤", "å®¿èˆ", "å…¬å¯“", "å›­åŒº", "ä½“è‚²é¦†"]):
                category = "Building"
            
            if category != "Other":
                category_counts[category] += 1
                try:
                    lng, lat = map(float, poi["location"].split(","))
                    classified_pois.append({
                        "id": f"P{len(classified_pois)+1:03d}",
                        "name": poi["name"],
                        "type": category,  # Canteen/Library/CafÃ©/Building
                        "lat": lat,
                        "lng": lng,
                        "rating": float(poi.get("biz_ext", {}).get("rating", 4.0) or 4.0),
                        "campus": poi["_campus"],
                        "address": poi.get("address", ""),
                        "tel": poi.get("tel", "")
                    })
                except (ValueError, AttributeError):
                    continue
            else:
                # Also add buildings that didn't match specific keywords but have 'Building' type
                pass
        
        print(f"  åˆ†å¸ƒ: {category_counts}")
        
        # Step 4: Generate road network
        print(f"\nğŸ›¤ï¸ ç”Ÿæˆè·¯ç½‘...")
        nodes = []
        for campus_name, coords in [("æ€æ˜", siming_coords), ("ç¿”å®‰", xiangan_coords)]:
            campus_pois_for_nodes = [p for p in classified_pois if p["campus"] == campus_name]
            campus_nodes = generate_nodes_from_pois(
                [{"name": p["name"], "location": f"{p['lng']},{p['lat']}"} for p in campus_pois_for_nodes],
                campus_name
            )
            nodes.extend(campus_nodes)
        
        edges = generate_edges(nodes, max_distance=500) # Increased connection distance
        edges = ensure_connectivity(nodes, edges)
        
        # Link POIs to nearest nodes
        for poi in classified_pois:
            min_dist = float('inf')
            nearest = None
            for node in nodes:
                if node["campus"] == poi["campus"]:
                    dist = haversine_distance(poi["lat"], poi["lng"], node["lat"], node["lng"])
                    if dist < min_dist:
                        min_dist = dist
                        nearest = node["id"]
            poi["nearest_node"] = nearest
        
        print(f"  Nodes: {len(nodes)}, Edges: {len(edges)}")
        
        # Step 5: Save data
        print(f"\nğŸ’¾ ä¿å­˜æ•°æ®...")
        output_dir = os.path.join(os.path.dirname(__file__), "..", "data", "campus")
        output_dir = os.path.abspath(output_dir)
        os.makedirs(output_dir, exist_ok=True)
        
        # Remove internal fields before saving
        clean_nodes = [{k: v for k, v in n.items()} for n in nodes]
        clean_pois = [{k: v for k, v in p.items() if not k.startswith("_")} for p in classified_pois]
        
        with open(os.path.join(output_dir, "nodes.json"), "w", encoding="utf-8") as f:
            json.dump({"nodes": clean_nodes}, f, ensure_ascii=False, indent=2)
        
        with open(os.path.join(output_dir, "edges.json"), "w", encoding="utf-8") as f:
            json.dump({"edges": edges}, f, ensure_ascii=False, indent=2)
        
        with open(os.path.join(output_dir, "pois.json"), "w", encoding="utf-8") as f:
            json.dump({"pois": clean_pois}, f, ensure_ascii=False, indent=2)
        
        # Save campus metadata
        with open(os.path.join(output_dir, "campuses.json"), "w", encoding="utf-8") as f:
            json.dump({
                "campuses": [
                    {"name": "æ€æ˜æ ¡åŒº", "lat": siming_coords[0], "lng": siming_coords[1]},
                    {"name": "ç¿”å®‰æ ¡åŒº", "lat": xiangan_coords[0], "lng": xiangan_coords[1]}
                ]
            }, f, ensure_ascii=False, indent=2)
        
        print(f"  âœ… æ•°æ®å·²ä¿å­˜åˆ° {output_dir}")
        print(f"  POIs: {len(classified_pois)} (ç¿”å®‰: {len([p for p in classified_pois if p['campus'] == 'ç¿”å®‰'])})")

if __name__ == "__main__":
    asyncio.run(main())
